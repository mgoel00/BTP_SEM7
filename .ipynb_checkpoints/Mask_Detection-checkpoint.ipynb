{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection using MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining our data generators\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "img_rows, img_cols = 128,128\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = './dataset/train'\n",
    "validation_data_dir = './dataset/validation'\n",
    "\n",
    "# Let's use some data augmentation and define our generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "train_images = train_generator.samples\n",
    "validation_images = validation_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We will use the pretrained MobileNet to which we will add our own network of FC layers.\n",
    "    We then train the new model formed for the additional layers.\n",
    "\n",
    "\"\"\"\n",
    "from keras.applications import MobileNet\n",
    "\n",
    "# Lets load the MobileNet model without the top or FC layers\n",
    "MobileNet = MobileNet(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Layers are set to be trainable as True by default but lets make them untrainable\n",
    "for layer in MobileNet.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's check our layers \n",
    "for (i,layer) in enumerate(MobileNet.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModelMobileNet(bottom_model):\n",
    "    \"\"\"\n",
    "    \n",
    "        Creates the head of the model that will be \n",
    "        placed ontop of the bottom layers\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(512,activation='relu')(top_model)\n",
    "    top_model = Dense(128,activation='relu')(top_model)\n",
    "    top_model = Dense(64,activation='relu')(top_model)\n",
    "    top_model = Dense(2,activation='sigmoid')(top_model)\n",
    "    \n",
    "    return top_model\n",
    "\n",
    "\n",
    "#Combining the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "FC_Head = addTopModelMobileNet(MobileNet)\n",
    "\n",
    "model = Model(inputs = MobileNet.input, outputs = FC_Head)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the mask detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our model\n",
    "#One could do some tweakings!\n",
    "from keras.optimizers import RMSprop, SGD,Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"face_mask_detector.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "nb_train_samples = train_images\n",
    "nb_validation_samples = validation_images\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'rmsprop',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the mask detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mask():\n",
    "    #Test on realtime video\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    import os\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from keras.models import load_model\n",
    "    return_label = ''\n",
    "    return_frame = ''\n",
    "    classifier = load_model('face_mask_detector.h5')\n",
    "    img_rows, img_cols = 128,128\n",
    "    # A function that puts the predicted class lables on the parametric image frames\n",
    "    def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_scale=0.8, thickness=1):\n",
    "        size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "        x, y = point\n",
    "        cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "        cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    #Define our prediction dictionary\n",
    "    face_classes = {0: 'with_mask', 1: 'without_mask'}\n",
    "    img_size = 128\n",
    "\n",
    "    detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        return_frame = frame\n",
    "        preprocessed_faces = []           \n",
    "\n",
    "        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = np.shape(frame)\n",
    "        detected = detector.detectMultiScale(frame)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "\n",
    "        if len(detected) > 0:\n",
    "\n",
    "            for f in detected:\n",
    "\n",
    "                # Obtain the coordinates of the detected face and draw a bounding box\n",
    "                x1, y1, w, h = [v for v in f]\n",
    "                cv2.rectangle(frame, (x1, y1), (x1+w,y1+h), (255, 0, 0), 2)\n",
    "                face =  frame[y1:y1+h, x1:x1+w, :]\n",
    "                face = cv2.resize(face, (img_rows,img_cols))\n",
    "                face = face.astype(\"float32\") / 255.0\n",
    "                face = np.expand_dims(face, axis=0)\n",
    "                preprocessed_faces.append(face)\n",
    "\n",
    "\n",
    "            # Make predictions for the detected face \n",
    "            face_labels = []\n",
    "            for i, d in enumerate(detected):\n",
    "                preds = classifier.predict(preprocessed_faces[i])[0]\n",
    "                face_labels.append(face_classes[np.argmax(preds,axis=0)])\n",
    "    #             print(preds)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Putting labels on frames\n",
    "            for i, d in enumerate(detected):\n",
    "                label = \"{}\".format(face_labels[i])\n",
    "                draw_label(frame, (x1,y1), label)\n",
    "                return_label = face_labels[i]\n",
    "                \n",
    "\n",
    "        #Display the results\n",
    "        \n",
    "        cv2.imshow(\"Face Mask Recognition\", frame)\n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return return_label,return_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #You can use some images for testing also using this part of the code\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # A function that puts the predicted class lables on the parametric image frames\n",
    "# def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                font_scale=0.8, thickness=1):\n",
    "#     size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "#     x, y = point\n",
    "#     cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "#     cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "# #Define our prediction dictionary\n",
    "# face_classes = {0: 'with_mask', 1: 'without_mask'}\n",
    "# img_size = 128\n",
    "\n",
    "# detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "# frame = cv2.imread('E:\\Others\\Shimla Manali\\IMG_20210910_134647.jpg')\n",
    "# frame = cv2.resize(frame,(1080,720))\n",
    "# preprocessed_faces = []           \n",
    "\n",
    "# input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "# img_h, img_w, _ = np.shape(frame)\n",
    "# detected = detector.detectMultiScale(frame)\n",
    "# faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "\n",
    "# if len(detected) > 0:\n",
    "\n",
    "#     for f in detected:\n",
    "\n",
    "#         # Obtain the coordinates of the detected face and draw a bounding box\n",
    "#         x1, y1, w, h = [v for v in f]\n",
    "#         cv2.rectangle(frame, (x1, y1), (x1+w,y1+h), (255, 0, 0), 2)\n",
    "#         face =  frame[y1:y1+h, x1:x1+w, :]\n",
    "#         face = cv2.resize(face, (img_rows,img_cols))\n",
    "#         face = face.astype(\"float32\") / 255.0\n",
    "#         face = np.expand_dims(face, axis=0)\n",
    "#         preprocessed_faces.append(face)\n",
    "\n",
    "\n",
    "#     # Make predictions for the detected face \n",
    "#     face_labels = []\n",
    "#     for i, d in enumerate(detected):\n",
    "#         preds = classifier.predict(preprocessed_faces[i])[0]\n",
    "#         face_labels.append(face_classes[np.argmax(preds,axis=0)])\n",
    "#         print(preds)\n",
    "\n",
    "#     # Putting labels on frames\n",
    "#     for i, d in enumerate(detected):\n",
    "#         label = \"{}\".format(face_labels[i])\n",
    "#         print(label)\n",
    "#         draw_label(frame, (x1,y1), label)\n",
    "\n",
    "# # Display the results\n",
    "# cv2.imshow(\"Face Recognition\", frame)\n",
    "# cv2.waitKey(0) == 13\n",
    "\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_face():\n",
    "    from imutils import paths\n",
    "    import face_recognition\n",
    "    import pickle\n",
    "    import cv2\n",
    "    import os\n",
    "\n",
    "    #get paths of each file in folder named Images\n",
    "    #Images here contains my data(folders of various persons)\n",
    "    imagePaths = list(paths.list_images('dataset_face'))\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        # load the input image and convert it from BGR (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #Use Face_recognition to locate faces\n",
    "        boxes = face_recognition.face_locations(rgb,model='hog')\n",
    "        # compute the facial embedding for the face\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        # loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "    #save emcodings along with their names in dictionary data\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    #use pickle to save data into a file for later use\n",
    "    f = open(\"face_enc\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    import face_recognition\n",
    "    import imutils\n",
    "    import pickle\n",
    "    import time\n",
    "    import cv2\n",
    "    import os\n",
    "\n",
    "    #find path of xml file containing haarcascade file\n",
    "    cascPathface = os.path.dirname(\n",
    "     cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "    # load the harcaascade in the cascade classifier\n",
    "    faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "    # load the known faces and embeddings saved in last file\n",
    "    data = pickle.loads(open('face_enc', \"rb\").read())\n",
    "    #Find path to the image you want to detect face and pass it here\n",
    "    image = frame\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #convert image to Greyscale for haarcascade\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # the facial embeddings for face in input\n",
    "    encodings = face_recognition.face_encodings(rgb)\n",
    "    names = []\n",
    "    # loop over the facial embeddings incase\n",
    "    # we have multiple embeddings for multiple fcaes\n",
    "    for encoding in encodings:\n",
    "        #Compare encodings with encodings in data[\"encodings\"]\n",
    "        #Matches contain array with boolean values and True for the embeddings it matches closely\n",
    "        #and False for rest\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "        encoding)\n",
    "        #set name =inknown if no encoding matches\n",
    "        name = \"Unknown\"\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            #Find positions at which we get True and store them\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                #Check the names at respective indexes we stored in matchedIdxs\n",
    "                name = data[\"names\"][i]\n",
    "                #increase count for the name we got\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "                #set name which has highest count\n",
    "                name = max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "            # update the list of names\n",
    "            names.append(name)\n",
    "            # loop over the recognized faces\n",
    "            for ((x, y, w, h), name) in zip(faces, names):\n",
    "                # rescale the face coordinates\n",
    "                # draw the predicted face name on the image\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(image, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                 0.75, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Frame\", image)\n",
    "        cv2.waitKey(0)\n",
    "        return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_mail(name):\n",
    "    mails = {'Mayank':'goel.mayank.13@gmail.com','Saksham':'saksham474011@gmail.com'}\n",
    "    import smtplib\n",
    "\n",
    "    gmail_user = 'tempcheckbtp@gmail.com'\n",
    "    gmail_password = 'Testing@123'\n",
    "\n",
    "    sent_from = gmail_user\n",
    "    to = mails[name]\n",
    "    subject = 'WARNING | FOUND VIOLATING COVID19 REGULATIONS'\n",
    "    body = 'Hey\\n You were found violating the COVID19 regulations in the work premises. Please contact your Manager.'\n",
    "\n",
    "    email_text = \"\"\"\\\n",
    "    From: %s\n",
    "    To: %s\n",
    "    Subject: %s\n",
    "\n",
    "    %s\n",
    "    \"\"\" % (sent_from, \", \".join(to), subject, body)\n",
    "\n",
    "    try:\n",
    "        smtp_server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "        smtp_server.ehlo()\n",
    "        smtp_server.login(gmail_user, gmail_password)\n",
    "        smtp_server.sendmail(sent_from, to, email_text)\n",
    "        smtp_server.close()\n",
    "        print (\"Email sent successfully!\")\n",
    "    except Exception as ex:\n",
    "        print (\"Something went wrong….\",ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking face\n",
      "Name  None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bf03c5e984f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Name \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msend_mail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "label,frame = detect_mask()\n",
    "# print(label)\n",
    "cv2.imwrite(\"frame.jpg\",frame)\n",
    "# detect_face_image(frame)\n",
    "# cv2.waitKey(0)\n",
    "if label == 'without_mask':\n",
    "    print(\"Checking face\")\n",
    "    names = detect_face(frame)\n",
    "    print(\"Name \",names)\n",
    "    send_mail(names[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
